{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "\n",
    "import hdf5storage\n",
    "InputDBY = hdf5storage.loadmat('InputDBY_6515.mat')\n",
    "InputDBY=InputDBY['InputDBY']\n",
    "\n",
    "SimulationData = hdf5storage.loadmat('SimulationData_6515.mat')\n",
    "LastIterline=SimulationData['LastIterline']\n",
    "LastIterRU=SimulationData['LastIterRU']\n",
    "LastIterRD=SimulationData['LastIterRD']\n",
    "\n",
    "InputDBY=np.transpose(InputDBY, (3,0,1,2))\n",
    "LastIterline=np.transpose(LastIterline, (2,0,1))\n",
    "LastIterRU=np.transpose(LastIterRU, (2,0,1))\n",
    "LastIterRD=np.transpose(LastIterRD, (2,0,1))\n",
    "\n",
    "InputDBY=np.float32(InputDBY)\n",
    "LastIterline=np.float32(LastIterline)\n",
    "LastIterRU=np.float32(LastIterRU)\n",
    "LastIterRD=np.float32(LastIterRD)\n",
    "\n",
    "print(InputDBY.shape)\n",
    "print(LastIterline.shape)\n",
    "print(LastIterRU.shape)\n",
    "print(LastIterRD.shape)\n",
    "\n",
    "InputDBY=InputDBY[0:3710,0:,0:,0:]\n",
    "LastIterline=LastIterline[0:3710,0:]\n",
    "LastIterRU=LastIterRU[0:3710,0:]\n",
    "LastIterRD=LastIterRD[0:3710,0:]\n",
    "\n",
    "print(InputDBY.shape)\n",
    "print(LastIterline.shape)\n",
    "print(LastIterRU.shape)\n",
    "print(LastIterRD.shape)\n",
    "\n",
    "Nunits=1388\n",
    "Nbus=6515\n",
    "Nbranch=9037\n",
    "Horizon=12\n",
    "\n",
    "n_scenario=3710\n",
    "n_train=2710\n",
    "n_test=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening target data\n",
    "LastIterline=np.reshape(LastIterline,(n_scenario,Nbranch*12),'F')\n",
    "LastIterRU=np.reshape(LastIterRU,(n_scenario,Nunits*11),'F')\n",
    "LastIterRD=np.reshape(LastIterRD,(n_scenario,Nunits*11),'F')\n",
    "\n",
    "print(LastIterline.shape)\n",
    "print(LastIterRU.shape)\n",
    "print(LastIterRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for training\n",
    "x_train=InputDBY[0:n_train,0:,0:,0:]\n",
    "y_train_line=LastIterline[0:n_train,0:]\n",
    "y_train_rampRU=LastIterRU[0:n_train,0:]\n",
    "y_train_rampRD=LastIterRD[0:n_train,0:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train_line.shape)\n",
    "print(y_train_rampRU.shape)\n",
    "print(y_train_rampRD.shape)\n",
    "\n",
    "#normalizing training data\n",
    "#finding the stat of training data\n",
    "maxD=np.amax(x_train[0:,0:,0:,0])\n",
    "maxC=np.amax(x_train[0:,0:,0:,1])\n",
    "maxT=np.amax(x_train[0:,0:,0:,2])\n",
    "\n",
    "x_train[0:,0:,0:,0]=x_train[0:,0:,0:,0]/maxD\n",
    "x_train[0:,0:,0:,1]=x_train[0:,0:,0:,1]/maxC\n",
    "#x_train[0:,0:,0:,2]=x_train[0:,0:,0:,2]/maxT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for testing\n",
    "x_test=InputDBY[n_train:,0:,0:,0:]\n",
    "y_test_line=LastIterline[n_train:,0:]\n",
    "y_test_rampRU=LastIterRU[n_train:,0:]\n",
    "y_test_rampRD=LastIterRD[n_train:,0:]\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test_line.shape)\n",
    "print(y_test_rampRU.shape)\n",
    "print(y_test_rampRD.shape)\n",
    "\n",
    "#normalizing testing data\n",
    "#finding the stat of testing data\n",
    "maxD=np.amax(x_test[0:,0:,0:,0])\n",
    "maxC=np.amax(x_test[0:,0:,0:,1])\n",
    "maxT=np.amax(x_test[0:,0:,0:,2])\n",
    "\n",
    "x_test[0:,0:,0:,0]=x_test[0:,0:,0:,0]/maxD\n",
    "x_test[0:,0:,0:,1]=x_test[0:,0:,0:,1]/maxC\n",
    "#x_test[0:,0:,0:,2]=x_test[0:,0:,0:,2]/maxT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def f2(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    f2 = 5*p*r / (4*p+r+K.epsilon())\n",
    "    return K.mean(f2)\n",
    "\n",
    "def f2_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    f2 = 5*p*r / (4*p+r+K.epsilon())\n",
    "    return 1 - K.mean(f2)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "custom_early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    min_delta=0.001, \n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "batch_size=100\n",
    "PreEpoc=10\n",
    "PostEpoch=30\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "base = EfficientNetB7(include_top=False, weights='imagenet',pooling='avg')\n",
    "\n",
    "#import efficientnet.tfkeras as enet\n",
    "#base = enet.EfficientNetB7(include_top=False, weights='imagenet',pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_line = Sequential()\n",
    "model_line.add(base)\n",
    "model_line.add(Flatten())\n",
    "model_line.add(Dense(1024, activation='relu'))\n",
    "model_line.add(Dense(Nbranch*12, activation='sigmoid'))\n",
    "\n",
    "for layer in model_line.layers[:-3]:\n",
    "    layer.trainable=False\n",
    "model_line.compile(loss=f2_loss, optimizer=Adam(), metrics=['accuracy',f2])\n",
    "history_line =model_line.fit(x_train, y_train_line, batch_size=batch_size, epochs=PreEpoc,verbose=1,validation_split=0.1)\n",
    "\n",
    "# Unfreeze the base model\n",
    "for layer in model_line.layers[-7:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model_line.compile(loss=f2_loss, optimizer=Adam(lr), metrics=['accuracy',f2])   \n",
    "history_line =model_line.fit(x_train, y_train_line, batch_size=batch_size, epochs=PostEpoch,verbose=1,validation_split=0.1,callbacks=[custom_early_stopping])\n",
    "\n",
    "model_line.save(\"model_line_6515.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rampRU = Sequential()\n",
    "model_rampRU.add(base)\n",
    "model_rampRU.add(Flatten())\n",
    "model_rampRU.add(Dense(256, activation='relu'))\n",
    "model_rampRU.add(Dense(Nunits*11, activation='sigmoid'))\n",
    "\n",
    "for layer in model_rampRU.layers[:-3]:\n",
    "    layer.trainable=False\n",
    "model_rampRU.compile(loss=f2_loss, optimizer=Adam(), metrics=['accuracy',f2])\n",
    "history_rampRU =model_rampRU.fit(x_train, y_train_rampRU, batch_size=batch_size, epochs=PreEpoc,verbose=1,validation_split=0.1)\n",
    "\n",
    "# Unfreeze the base model\n",
    "for layer in model_rampRU.layers[-7:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model_rampRU.compile(loss=f2_loss, optimizer=Adam(lr), metrics=['accuracy',f2])\n",
    "history_rampRU =model_rampRU.fit(x_train, y_train_rampRU, batch_size=batch_size, epochs=PostEpoch,verbose=1,validation_split=0.1,callbacks=[custom_early_stopping])\n",
    "\n",
    "model_rampRU.save(\"model_rampRU_6515.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and finetunig the modified EfficientNet architecture with imagenet weights\n",
    "model_rampRD = Sequential()\n",
    "model_rampRD.add(base)\n",
    "model_rampRD.add(Flatten())\n",
    "model_rampRD.add(Dense(256, activation='relu'))\n",
    "model_rampRD.add(Dense(Nunits*11, activation='sigmoid'))\n",
    "\n",
    "for layer in model_rampRD.layers[:-3]:\n",
    "    layer.trainable=False\n",
    "model_rampRD.compile(loss=f2_loss, optimizer=Adam(), metrics=['accuracy',f2])\n",
    "history_rampRD =model_rampRD.fit(x_train, y_train_rampRD, batch_size=batch_size, epochs=PreEpoc,verbose=1,validation_split=0.1)\n",
    "\n",
    "# Unfreeze the base model\n",
    "for layer in model_rampRD.layers[-7:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model_rampRD.compile(loss=f2_loss, optimizer=Adam(lr), metrics=['accuracy',f2])\n",
    "history_rampRD =model_rampRD.fit(x_train, y_train_rampRD, batch_size=batch_size, epochs=PostEpoch,verbose=1,validation_split=0.1,callbacks=[custom_early_stopping])\n",
    "\n",
    "model_rampRD.save(\"model_rampRD_6515.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(history.history.keys())\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_line.history['loss'])\n",
    "plt.title('Model Training Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss_line'], loc='upper right')          \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_rampRU.history['loss'])\n",
    "plt.title('Model Training Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss_rampRU'], loc='upper right')          \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_rampRD.history['loss'])\n",
    "plt.title('Model Training Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss_rampRD'], loc='upper right')          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction \n",
    "y_pred_line=model_line.predict(x_test)\n",
    "y_pred_rampRU=model_rampRU.predict(x_test)\n",
    "y_pred_rampRD=model_rampRD.predict(x_test)\n",
    "print(y_pred_line.shape)\n",
    "print(y_pred_rampRU.shape)\n",
    "print(y_pred_rampRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "probability_fitering=np.array([0.999])\n",
    "\n",
    "#testing the Branch classifier\n",
    "y_test_line=np.reshape(y_test_line,np.size(y_test_line,0)*np.size(y_test_line,1),'c')\n",
    "y_pred_line=np.reshape(y_pred_line,np.size(y_pred_line,0)*np.size(y_pred_line,1),'c')\n",
    "\n",
    "y_pred_line[y_pred_line >= probability_fitering] = 1\n",
    "y_pred_line[y_pred_line < probability_fitering] = 0\n",
    "print(confusion_matrix(y_test_line, y_pred_line))\n",
    "\n",
    "#testing the Ramp Up classifier\n",
    "y_test_rampRU=np.reshape(y_test_rampRU,np.size(y_test_rampRU,0)*np.size(y_test_rampRU,1),'c')\n",
    "y_pred_rampRU=np.reshape(y_pred_rampRU,np.size(y_pred_rampRU,0)*np.size(y_pred_rampRU,1),'c')\n",
    "\n",
    "y_pred_rampRU[y_pred_rampRU >= probability_fitering] = 1\n",
    "y_pred_rampRU[y_pred_rampRU < probability_fitering] = 0\n",
    "print(confusion_matrix(y_test_rampRU, y_pred_rampRU))\n",
    "\n",
    "#testing the Ramp Down classifier\n",
    "y_test_rampRD=np.reshape(y_test_rampRD,np.size(y_test_rampRD,0)*np.size(y_test_rampRD,1),'c')\n",
    "y_pred_rampRD=np.reshape(y_pred_rampRD,np.size(y_pred_rampRD,0)*np.size(y_pred_rampRD,1),'c')\n",
    "\n",
    "y_pred_rampRD[y_pred_rampRD >= probability_fitering] = 1\n",
    "y_pred_rampRD[y_pred_rampRD < probability_fitering] = 0\n",
    "print(confusion_matrix(y_test_rampRD, y_pred_rampRD))\n",
    "\n",
    "print(y_pred_line.shape)\n",
    "print(y_pred_rampRU.shape)\n",
    "print(y_pred_rampRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouput data reshaping\n",
    "y_pred_line=np.reshape(y_pred_line,(n_test,Nbranch*12),'c')\n",
    "y_pred_rampRU=np.reshape(y_pred_rampRU,(n_test,Nunits*11),'c')\n",
    "y_pred_rampRD=np.reshape(y_pred_rampRD,(n_test,Nunits*11),'c')\n",
    "print(y_pred_line.shape)\n",
    "print(y_pred_rampRU.shape)\n",
    "print(y_pred_rampRD.shape)\n",
    "\n",
    "#Breaking each scenario into row and columns\n",
    "y_pred_line=np.reshape(y_pred_line,(n_test,Nbranch,12),'F')\n",
    "y_pred_rampRU=np.reshape(y_pred_rampRU,(n_test,Nunits,11),'F')\n",
    "y_pred_rampRD=np.reshape(y_pred_rampRD,(n_test,Nunits,11),'F')\n",
    "print(y_pred_line.shape)\n",
    "print(y_pred_rampRU.shape)\n",
    "print(y_pred_rampRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
