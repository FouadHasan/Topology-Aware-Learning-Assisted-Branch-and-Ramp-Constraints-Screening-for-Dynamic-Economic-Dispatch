{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "# read the image stack\n",
    "# Original values in tif images are scaled between 0-255\n",
    "demand_img = io.imread('inputDBY_118.tif')\n",
    "demand_img = np.array(demand_img)\n",
    "print(demand_img.shape)\n",
    "\n",
    "LastIterLine_img = io.imread('LastIterLine_118.tif')\n",
    "LastIterLine_img = np.array(LastIterLine_img)\n",
    "print(LastIterLine_img.shape)\n",
    "\n",
    "LastIterRU_img = io.imread('LastIterRU_118.tif')\n",
    "LastIterRU_img = np.array(LastIterRU_img)\n",
    "print(LastIterRU_img.shape)\n",
    "\n",
    "LastIterRD_img = io.imread('LastIterRD_118.tif')\n",
    "LastIterRD_img = np.array(LastIterRD_img)\n",
    "print(LastIterRD_img.shape)\n",
    "\n",
    "Nunits=54\n",
    "Nbus=118\n",
    "Nbranch=186\n",
    "Horizon=24\n",
    "#n_scenario=np.size(demand_img,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing between 0-1\n",
    "demand_img=(demand_img/255)\n",
    "LastIterLine_img=(LastIterLine_img/255)   \n",
    "LastIterRU_img=(LastIterRU_img/255)\n",
    "LastIterRD_img=(LastIterRD_img/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to original values\n",
    "#as input was converted to images (0 to 255)\n",
    "maxDem=518.1735\n",
    "maxcostB=43.3511\n",
    "maxYbus=385.9456\n",
    "#maxcostA=0.0802\n",
    "\n",
    "demand_img[0:,0:,0:,0]=demand_img[0:,0:,0:,0]*maxDem\n",
    "demand_img[0:,0:,0:,1]=demand_img[0:,0:,0:,1]*maxcostB\n",
    "demand_img[0:,0:,0:,2]=demand_img[0:,0:,0:,2]*maxYbus\n",
    "#demand_img[0:,0:,0:,1]=demand_img[0:,0:,0:,1]*maxcostA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for training\n",
    "\n",
    "demand_img_train=demand_img[0:4700,0:,0:,0:]\n",
    "LastIterLine_img_train=LastIterLine_img[0:4700,0:,0:]\n",
    "LastIterRU_img_train=LastIterRU_img[0:4700,0:,0:]\n",
    "LastIterRD_img_train=LastIterRD_img[0:4700,0:,0:]\n",
    "print(demand_img_train.shape)\n",
    "print(LastIterLine_img_train.shape)\n",
    "print(LastIterRU_img_train.shape)\n",
    "print(LastIterRD_img_train.shape)\n",
    "\n",
    "#normalizing training data\n",
    "#finding the stat of training data\n",
    "maxD=np.amax(demand_img_train[0:,0:,0:,0])\n",
    "maxB=np.amax(demand_img_train[0:,0:,0:,1])\n",
    "maxY=np.amax(demand_img_train[0:,0:,0:,2])\n",
    "#maxA=np.amax(demand_img_train[0:,0:,0:,2])\n",
    "\n",
    "demand_img_train[0:,0:,0:,0]=demand_img_train[0:,0:,0:,0]/maxD\n",
    "demand_img_train[0:,0:,0:,1]=demand_img_train[0:,0:,0:,1]/maxB\n",
    "demand_img_train[0:,0:,0:,2]=demand_img_train[0:,0:,0:,2]/maxY\n",
    "#demand_img[0:,0:,0:,2]=demand_img[0:,0:,0:,2]/maxA #this channel is zero for linear cost (0 division)\n",
    "\n",
    "#Reshaping training data\n",
    "n_scenario=np.size(demand_img_train,0)\n",
    "#n_training_scenario=np.size(demand_img_train,0)\n",
    "x_train_demand_line=np.reshape(demand_img_train,(n_scenario,Nbus,32,3))\n",
    "x_train_demand_rampRU=np.copy(x_train_demand_line)\n",
    "x_train_demand_rampRD=np.copy(x_train_demand_line)\n",
    "y_train_line=np.reshape(LastIterLine_img_train,(n_scenario,Nbranch*24),'F')\n",
    "y_train_rampRU=np.reshape(LastIterRU_img_train,(n_scenario,Nunits*23),'F')\n",
    "y_train_rampRD=np.reshape(LastIterRD_img_train,(n_scenario,Nunits*23),'F')\n",
    "\n",
    "print(x_train_demand_line.shape)\n",
    "print(y_train_line.shape)\n",
    "print(y_train_rampRU.shape)\n",
    "print(y_train_rampRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for testing\n",
    "\n",
    "demand_img_test=demand_img[4700:,0:,0:,0:]\n",
    "LastIterLine_img_test=LastIterLine_img[4700:,0:,0:]\n",
    "LastIterRU_img_test=LastIterRU_img[4700:,0:,0:]\n",
    "LastIterRD_img_test=LastIterRD_img[4700:,0:,0:]\n",
    "print(demand_img_test.shape)\n",
    "print(LastIterLine_img_test.shape)\n",
    "print(LastIterRU_img_test.shape)\n",
    "print(LastIterRD_img_test.shape)\n",
    "\n",
    "#normalizing testing data\n",
    "#finding the stat of testing data\n",
    "maxD=np.amax(demand_img_test[0:,0:,0:,0])\n",
    "maxB=np.amax(demand_img_test[0:,0:,0:,1])\n",
    "maxY=np.amax(demand_img_test[0:,0:,0:,2])\n",
    "#maxA=np.amax(demand_img_test[0:,0:,0:,2])\n",
    "\n",
    "demand_img_test[0:,0:,0:,0]=demand_img_test[0:,0:,0:,0]/maxD\n",
    "demand_img_test[0:,0:,0:,1]=demand_img_test[0:,0:,0:,1]/maxB\n",
    "demand_img_test[0:,0:,0:,2]=demand_img_test[0:,0:,0:,2]/maxY\n",
    "\n",
    "#Reshaping testing data\n",
    "n_scenario=np.size(demand_img_test,0)\n",
    "#n_test_scenario=np.size(demand_img_test,0)\n",
    "x_test_demand_line=np.reshape(demand_img_test,(n_scenario,Nbus,32,3))\n",
    "x_test_demand_rampRU=np.copy(x_test_demand_line)\n",
    "x_test_demand_rampRD=np.copy(x_test_demand_line)\n",
    "y_test_line=np.reshape(LastIterLine_img_test,(n_scenario,Nbranch*24),'F')\n",
    "y_test_rampRU=np.reshape(LastIterRU_img_test,(n_scenario,Nunits*23),'F')\n",
    "y_test_rampRD=np.reshape(LastIterRD_img_test,(n_scenario,Nunits*23),'F')\n",
    "\n",
    "n_test_scenario=np.size(demand_img_test,0)\n",
    "\n",
    "print(x_test_demand_line.shape)\n",
    "print(y_test_line.shape)\n",
    "print(y_test_rampRU.shape)\n",
    "print(y_test_rampRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def f2(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    f2 = 5*p*r / (4*p+r+K.epsilon())\n",
    "    return K.mean(f2)\n",
    "\n",
    "def f2_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    f2 = 5*p*r / (4*p+r+K.epsilon())\n",
    "    return 1 - K.mean(f2)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "custom_early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    min_delta=0.001, \n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing EfficientNet Architecture with imagenet weights\n",
    "import efficientnet.tfkeras as enet\n",
    "base = enet.EfficientNetB7(include_top=False, weights='imagenet',pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training and finetunig the modified EfficientNet architecture with imagenet weights\n",
    "model_line = Sequential()\n",
    "model_line.add(base)\n",
    "model_line.add(Flatten())\n",
    "model_line.add(Dense(4096, activation='relu'))\n",
    "model_line.add(Dense(Nbranch*24, activation='sigmoid'))\n",
    "\n",
    "for layer in model_line.layers[:-3]:\n",
    "    layer.trainable=False\n",
    "model_line.compile(loss=f2_loss, optimizer=Adam(), metrics=['accuracy',f2])\n",
    "history_line =model_line.fit(x_train_demand_line, y_train_line, batch_size=50, epochs=10,verbose=1,validation_split=0.1)\n",
    "\n",
    "# Unfreeze the base model\n",
    "for layer in model_line.layers[-5:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model_line.compile(loss=f2_loss, optimizer=Adam(0.00001), metrics=['accuracy',f2])   \n",
    "history_line =model_line.fit(x_train_demand_line, y_train_line, batch_size=50, epochs=30,verbose=1,validation_split=0.1,callbacks=[custom_early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and finetunig the modified EfficientNet architecture with imagenet weights\n",
    "\n",
    "model_rampRU = Sequential()\n",
    "model_rampRU.add(base)\n",
    "model_rampRU.add(Flatten())\n",
    "model_rampRU.add(Dense(1024, activation='relu'))\n",
    "model_rampRU.add(Dense(Nunits*23, activation='sigmoid'))\n",
    "\n",
    "for layer in model_rampRU.layers[:-3]:\n",
    "    layer.trainable=False\n",
    "model_rampRU.compile(loss=f2_loss, optimizer=Adam(), metrics=['accuracy',f2])\n",
    "history_rampRU =model_rampRU.fit(x_train_demand_rampRU, y_train_rampRU, batch_size=50, epochs=10,verbose=1,validation_split=0.1)\n",
    "\n",
    "# Unfreeze the base model\n",
    "for layer in model_rampRU.layers[-5:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model_rampRU.compile(loss=f2_loss, optimizer=Adam(0.00001), metrics=['accuracy',f2])\n",
    "history_rampRU =model_rampRU.fit(x_train_demand_rampRU, y_train_rampRU, batch_size=50, epochs=30,verbose=1,validation_split=0.1,callbacks=[custom_early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and finetunig the modified EfficientNet architecture with imagenet weights\n",
    "\n",
    "model_rampRD = Sequential()\n",
    "model_rampRD.add(base)\n",
    "model_rampRD.add(Flatten())\n",
    "model_rampRD.add(Dense(1024, activation='relu'))\n",
    "model_rampRD.add(Dense(Nunits*23, activation='sigmoid'))\n",
    "\n",
    "for layer in model_rampRD.layers[:-3]:\n",
    "    layer.trainable=False\n",
    "model_rampRD.compile(loss=f2_loss, optimizer=Adam(), metrics=['accuracy',f2])\n",
    "history_rampRD =model_rampRD.fit(x_train_demand_rampRD, y_train_rampRD, batch_size=50, epochs=10,verbose=1,validation_split=0.1)\n",
    "\n",
    "# Unfreeze the base model\n",
    "for layer in model_rampRD.layers[-5:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model_rampRD.compile(loss=f2_loss, optimizer=Adam(0.00001), metrics=['accuracy',f2])\n",
    "history_rampRD =model_rampRD.fit(x_train_demand_rampRD, y_train_rampRD, batch_size=50, epochs=30,verbose=1,validation_split=0.1,callbacks=[custom_early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(history.history.keys())\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_line.history['loss'])\n",
    "plt.title('Model Training Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss_line'], loc='upper right')          \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_rampRU.history['loss'])\n",
    "plt.title('Model Training Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss_rampRU'], loc='upper right')          \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_rampRD.history['loss'])\n",
    "plt.title('Model Training Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss_rampRD'], loc='upper right')          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction \n",
    "y_pred_line=model_line.predict(x_test_demand_line)\n",
    "y_pred_rampRU=model_rampRU.predict(x_test_demand_rampRU)\n",
    "y_pred_rampRD=model_rampRD.predict(x_test_demand_rampRD)\n",
    "print(y_pred_line.shape)\n",
    "print(y_pred_rampRU.shape)\n",
    "print(y_pred_rampRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#testing the Branch classifier\n",
    "y_test=np.reshape(y_test_line,np.size(y_test_line,0)*np.size(y_test_line,1),'c')\n",
    "y_pred=np.reshape(y_pred_line,np.size(y_test_line,0)*np.size(y_test_line,1),'c')\n",
    "\n",
    "probability_fitering=np.array([0.999])\n",
    "\n",
    "y_predline=np.copy(y_pred)\n",
    "y_predline[y_predline >= probability_fitering] = 1\n",
    "y_predline[y_predline < probability_fitering] = 0\n",
    "print(confusion_matrix(y_test, y_predline))\n",
    "\n",
    "#testing the Ramp Up classifier\n",
    "y_test=np.reshape(y_test_rampRU,np.size(y_test_rampRU,0)*np.size(y_test_rampRU,1),'c')\n",
    "y_pred=np.reshape(y_pred_rampRU,np.size(y_test_rampRU,0)*np.size(y_test_rampRU,1),'c')\n",
    "\n",
    "y_predRU=np.copy(y_pred)\n",
    "y_predRU[y_predRU >= probability_fitering] = 1\n",
    "y_predRU[y_predRU < probability_fitering] = 0\n",
    "print(confusion_matrix(y_test, y_predRU))\n",
    "\n",
    "#testing the Ramp Down classifier\n",
    "y_test=np.reshape(y_test_rampRD,np.size(y_test_rampRD,0)*np.size(y_test_rampRD,1),'c')\n",
    "y_pred=np.reshape(y_pred_rampRD,np.size(y_test_rampRD,0)*np.size(y_test_rampRD,1),'c')\n",
    "\n",
    "y_predRD=np.copy(y_pred)\n",
    "y_predRD[y_predRD >= probability_fitering] = 1\n",
    "y_predRD[y_predRD < probability_fitering] = 0\n",
    "print(confusion_matrix(y_test, y_predRD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouput data reshaping\n",
    "n_test_scenario=np.size(demand_img_test,0)\n",
    "y_predline=np.reshape(y_predline,(n_test_scenario,Nbranch*24),'c')\n",
    "y_predRU=np.reshape(y_predRU,(n_test_scenario,Nunits*23),'c')\n",
    "y_predRD=np.reshape(y_predRD,(n_test_scenario,Nunits*23),'c')\n",
    "print(y_predline.shape)\n",
    "print(y_predRU.shape)\n",
    "print(y_predRD.shape)\n",
    "\n",
    "#Breaking each scenario into row and columns\n",
    "y_predline=np.reshape(y_predline,(n_test_scenario,Nbranch,24),'F')\n",
    "y_predRU=np.reshape(y_predRU,(n_test_scenario,Nunits,23),'F')\n",
    "y_predRD=np.reshape(y_predRD,(n_test_scenario,Nunits,23),'F')\n",
    "print(y_predline.shape)\n",
    "print(y_predRU.shape)\n",
    "print(y_predRD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the trained model\n",
    "model_line.save(\"model_line_118_Last.h5\")\n",
    "model_rampRU.save(\"model_rampRU_118_Last.h5\")\n",
    "model_rampRD.save(\"model_rampRD_118_Last.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the predicted result back to image format\n",
    "from skimage import io\n",
    "io.imsave('predicted_branch_const_118_Last.tif',y_predline)\n",
    "io.imsave('predicted_rampRU_const_118_Last.tif',y_predRU)\n",
    "io.imsave('predicted_rampRD_const_118_Last.tif',y_predRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
